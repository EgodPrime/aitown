# 十一、非功能需求、风险与实施计划

非功能需求
- 性能：在单台机器上支持 50 个 NPC 并保证每 2 秒更新一次不导致后端明显阻塞。若使用真实 OpenAI API，需考虑调用延迟与并发配额；优先使用批量或并发限制策略并回退到本地规则以保证仿真节奏。
- 可用性：前端在断开重连后能拉取 full_state 并继续渲染
- 可维护性：LLM 适配器应采用插件式设计，便于替换

风险与缓解措施
- 风险：LLM API 调用延迟或失败 → 缓解：超时回退到本地规则/默认行为；若玩家提供私有 API，需验证其可用性并在失败时回退到服务器 LLM
- 风险：WebSocket 广播成为瓶颈 → 缓解：实施房间/分片广播或使用消息代理

实施计划（里程碑）
- Week 0: 需求确认与 PRD（当前）
- Week 1: MVP 实现（后端仿真、API、前端可视化）
- Week 2: LLM Adapter 插件化、简单持久化
- Week 3: 前端增强（编辑、交互、UI 改进）

开放问题（需要 PO/PM 确认）
1. 是否需要社交登录或多人协作的编辑权限控制？
2. 是否允许玩家导入/导出 NPC 配置？
3. 是否需要内置示例 prompt 库供新手使用？
